Evaluating against a single measure


#資料來源 https://discourse.juliacn.com/t/topic/4025/2
●單一測量使用範例

using MLJ

X = (a=rand(12), b=rand(12), c=rand(12));

y = X.a + 2X.b + 0.05*rand(12);

●訓練模型
model = (@load RidgeRegressor pkg=MultivariateStats verbosity=0)()
#RidgeRegressor 回歸任務模型  L2均方误差 (Mean Squared Error, MSE)
 
rng=StableRNG(1234)
#固定隨機生成器
cv = CV(nfolds = 3, shuffle = true,rng=rng) 
#重采样策略 shuffle在分割数据集之前，将数据集随机打乱
# cross-validation 交叉驗證 分成三份

evaluate(model, X, y, resampling=cv, measure=l2, verbosity=0)
# L2均方误差 (Mean Squared Error, MSE)

等同於

ach = machine(model, X, y)
evaluate!(mach, resampling = cv, measure = l2 ,rng=rng)

輸出:
_.measure	_.measurement	_.per_fold
l2	0.164	[0.105, 0.23, 0.158]

● 多個measure

evaluate(model, X, y, resampling = cv, measure = [l1, rms, rmslp1],rng=rng)




● measure 參數

☐回歸:
►L2 :均方误差 (Mean Squared Error, MSE) (有除以n)

►l1 :絕對誤差之和（Sum of Absolute Errors, SAE）(有除以n)

#∑_{i=1}^{n} |y_i - \hat{y}_i|

►mae：平均绝对误差

r2：决定系数（R-squared, R²）
#用于评估模型对数据方差的解释能力。 R ^2 =(1-   残差變異(表示所有观测值相对于预测值的残差变异)/總變異(表示所有观测值相对于平均值的总变异))

☐分類任務

►accuracy：準確率 = Number of Correct Predictions/Total Number of Predictions
​
►auc：ROC 曲线下面积（Area Under the Curve）
參考:https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/learning-model-%E4%BB%80%E9%BA%BC%E6%98%AFroc%E5%92%8Cauc-%E8%BD%89%E9%8C%84-33aafe644cf

►log_loss：对数损失（Logarithmic Loss）(常用於二元分類)
Log Loss = -(1/n) * Σ [y_i * log(y_hat_i) + (1 - y_i) * log(1 - y_hat_i)], for i = 1 to n

►precision：精确率  True Positives/(True Positives+False Positives)


►recall：召回率 True Positives/ (True Positives+False Negatives)

►f1：F1 分数 F1 = 2 * P*R/(P+R)



●评估模型的必要参数
☐resampling
内置重采样策略有三個， Holdout, CV 与 StratifiedCV

►Holdout
其实就跟sklearn里的train_test_split差不多，将训练集和测试集按一定比例划分

holdout = Holdout(; fraction_train=0.7,
                   	shuffle=nothing,
					rng=nothing)

►.CV
交叉验证重采样策略

cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)


►StratifiedCV
分层交叉验证重采样策略,仅适用于分类问题（OrderedFactor或Multiclass目标）(它确保每个折中的类别比例与原始数据集中相同)

stratified_cv = StratifiedCV(; nfolds=6,
                               shuffle=false,
                               rng=Random.GLOBAL_RNG)

☐ measure
☐ 分类指标
►混淆矩阵
Ground	Truth
Predicted	Positive	Negative
True	TP	FN
False	FP	TN
由混淆矩阵推导出的概率

►准确率
►精确率
►召回率
[补充] FScore为精确率与召回率的调和平均

我太懒了，别人比我总结的好，看这篇文章吧

https://zhuanlan.zhihu.com/p/46714763
2.2.2 回归指标
►l1
∑|(Yᵢ - h(xᵢ)|
►l2
∑(Yᵢ - h(xᵢ))²
►mae
平均绝对误差
l1(Ŷ,h(xᵢ)) / n
►mse
平均平方误差
l2(Ŷ,h(xᵢ)) / n
►rmse
均方根误差
√(∑(ŷ - y)²



►擴展包 LossFunction
https://github.com/JuliaML/LossFunctions.jl

using LossFunctions

X = (x1=rand(5), x2=rand(5));
y = categorical(["y", "y", "y", "n", "y"]);
w = [1, 2, 1, 2, 3];

mach = machine(ConstantClassifier(), X, y);
holdout = Holdout(fraction_train=0.6);
evaluate!(mach,
          measure=[ZeroOneLoss(), L1HingeLoss(), L2HingeLoss(), SigmoidLoss()],
          resampling=holdout,
          operation=predict,
          weights=w)



●评估模型的图表

►curve = learning_curve(mach; resolution=30,
                               resampling=Holdout(),
                               repeats=1,
                               measure=default_measure(machine.model),
                               rows=nothing,
                               weights=nothing,
                               operation=predict,
                               range=nothing,
                               acceleration=default_resource(),
                               acceleration_grid=CPU1(),
                               rngs=nothing,
                               rng_name=nothing)
其实只是名字一样，给定一个范围range(only one)，得到一个曲线curve，这个曲线表示这个范围内的所有性能（指标）
上面那个resolution=30说明learning_curve使用Grid作获取参数的策略



►example 观察一个模型的性能

X, y = @load_boston

@load RidgeRegressor pkg=MultivariateStats
model = RidgeRegressor()
mach = machine(model, X, y)

r_lambda = range(model, :lambda, lower = 0.01, upper = 10.0, scale = :linear)
默认重采样策略Holdout

curves = learning_curve(mach,
                        range = r_lambda,
                        measure = rms)
plot(curves.parameter_values,
     curves.measurements,
     xlab = curves.parameter_name,
     ylab = "Holdout estimate of RMS error")



►指定重采样策略

using Plots
rng = StableRNG(1234)

curves = learning_curve(mach,
                        resampling = CV(nfolds = 6, rng = rng),
                        range = r_lambda,
                        measure = rms)
plot(curves.parameter_values,
     curves.measurements,
     xlab = curves.parameter_name,
     ylab = "Holdout estimate of RMS error")


► ROC
fprs, tprs, ts = roc_curve(ŷ, y) = roc(ŷ, y)
如果我们的测试数据集类别分布大致均衡的时候我们可以用ROC曲线
给定基本事实y，两类概率预测ŷ，返回ROC曲线。ts返回阈值范围内的真阳性率，假阳性率
其中

fprs: 假阳性率
tprs: 真阳性率
ts: thresholds 阈值
example
这里我不给出代码了，因为roc曲线评估的是分类问题，加载数据集，处理那些操作太多了，我就先放图片好了，具体的流程在Titanic幸存预测里



►真正的学习曲线
虽然我找不到这个实现，但是我自己先做了一个

function plot_learning_curve(model, X, y)
    mach = machine(model, X, y)
    training_size_iter = 5:10:length(y)
    errors = ones(length(training_size_iter), 2)
    rng = StableRNG(1234)

    row = 1                     # for iterate
    for training_size = training_size_iter
        train, cv = partition(1:training_size, 0.7, rng = rng)
        fit_only!(mach, rows = train)
        
        m_train = length(train)
        Jtrain = (1 / (2 * m_train)) * reduce(+, map(x -> x^2, predict(mach, rows = train) - y[train]))

        m_cv = length(cv)
        Jcv = (1 / (2 * m_cv)) * reduce(+, map(x -> x^2, predict(mach, rows = cv) - y[cv]))

        errors[row, :] = [Jtrain, Jcv]

        row += 1
    end

    plot(errors,
         label = ["Jtrain" "Jcv"],
         color = [:red :blue],
         xlab = "training size",
         ylab = "error")


end
试试看

@load RidgeRegressor pkg=MultivariateStats
model = RidgeRegressor()
X, y = @load_boston

# Tuning
rng = StableRNG(1234)
r_lambda = range(model, :lambda, lower = 0.1, upper = 10.0, scale = :linear)
tuning = Grid(resolution = 100, rng = rng)
resampling = CV(nfolds = 6, rng = rng)
self_tuning_model = TunedModel(model = model,
                               range = r_lambda,
                               tuning = tuning,
                               resampling = resampling,
                               measure = l1)
self_tuning_mach = machine(self_tuning_model, X, y)
fit!(self_tuning_mach, force = true)

best_model = fitted_params(self_tuning_mach).best_model

plot_learning_curve(best_model, X, y)


●結束





